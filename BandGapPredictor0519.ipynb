{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2124ca8e",
   "metadata": {},
   "source": [
    "# 这里包含了两类计算：分类和回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8211817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#引入需要的python库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "import pymatgen as mg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f1340",
   "metadata": {},
   "source": [
    "# 例一：分类计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dad9f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data 读取数据\n",
    "DE_c = pd.read_excel('Training_Set.xlsx',sheet_name=0)\n",
    "array_c = DE_c.values   #数据数值提取\n",
    "X_c = array_c[:,3:139]  #第3—139列数据作为输入变量X\n",
    "Y_c = array_c[:,2]      #第2列作为目标数据Y\n",
    "Y_c = Y_c.astype('int') #强制将目标数据的类型转换为整型int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9af09aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test split\n",
    "X_train_c, X_test_c, Y_train_c, Y_test_c = train_test_split(X_c, Y_c, test_size=0.1, random_state=15, shuffle=True)\n",
    "\n",
    "\n",
    "# train_test_split函数的简单说明\n",
    "#在机器学习中，我们通常将原始数据按照比例分割为“测试集”和“训练集”，从 sklearn.model_selection 中调用train_test_split 函数 \n",
    "\n",
    "#X_train,X_test, y_train, y_test =s klearn.model_selection.train_test_split(train_data,train_target,test_size=0.4, random_state=0,stratify=y_train)\n",
    "\n",
    "# train_data：所要划分的样本特征集\n",
    "\n",
    "# train_target：所要划分的样本结果\n",
    "\n",
    "# test_size：样本占比，如果是整数的话就是样本的数量\n",
    "\n",
    "# random_state：是随机数的种子。\n",
    "# 随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。\n",
    "\n",
    "#stratify是为了保持split前类的分布。\n",
    "#比如有100个数据，80个属于A类，20个属于B类。如果train_test_split(... test_size=0.25, stratify = y_all), 那么split之后数据如下： \n",
    "#training: 75个数据，其中60个属于A类，15个属于B类。 \n",
    "#testing: 25个数据，其中20个属于A类，5个属于B类。 \n",
    "\n",
    "#用了stratify参数，training集和testing集的类的比例是 A：B= 4：1，等同于split前的比例（80：20）。通常在这种类分布不平衡的情况下会用到stratify。\n",
    "\n",
    "#将stratify=X就是按照X中的比例分配 \n",
    "\n",
    "#将stratify=y就是按照y中的比例分配 \n",
    "\n",
    "#整体总结起来各个参数的设置及其类型如下：\n",
    "\n",
    "#主要参数说明：\n",
    "\n",
    "#*arrays：可以是列表、numpy数组、scipy稀疏矩阵或pandas的数据框\n",
    "\n",
    "#test_size：可以为浮点、整数或None，默认为None\n",
    "\n",
    "#①若为浮点时，表示测试集占总样本的百分比\n",
    "\n",
    "#②若为整数时，表示测试样本样本数\n",
    "\n",
    "#③若为None时，test size自动设置成0.25\n",
    "\n",
    "#train_size：可以为浮点、整数或None，默认为None\n",
    "\n",
    "#①若为浮点时，表示训练集占总样本的百分比\n",
    "\n",
    "#②若为整数时，表示训练样本的样本数\n",
    "\n",
    "#③若为None时，train_size自动被设置成0.75\n",
    "\n",
    "#random_state：可以为整数、RandomState实例或None，默认为None\n",
    "\n",
    "#①若为None时，每次生成的数据都是随机，可能不一样\n",
    "\n",
    "#②若为整数时，每次生成的数据都相同\n",
    "\n",
    "#stratify：可以为类似数组或None\n",
    "\n",
    "#①若为None时，划分出来的测试集或训练集中，其类标签的比例也是随机的\n",
    "\n",
    "#②若不为None时，划分出来的测试集或训练集中，其类标签的比例同输入的数组中类标签的比例相同，可以用于处理不均衡的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "506851af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing  预处理\n",
    "scaler_c = preprocessing.StandardScaler().fit(X_train_c)\n",
    "\n",
    "X_train_c = scaler_c.transform(X_train_c)\n",
    "#print(X_train_c)\n",
    "X_test_c = scaler_c.transform(X_test_c)\n",
    "#print(X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7037173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model construction\n",
    "classification=SVC(kernel='rbf',C=10**1.5, gamma= 0.01).fit(X_train_c, Y_train_c)\n",
    "#说明：\n",
    "\n",
    "#save model\n",
    "classification_model=pickle.dumps(classification)\n",
    "\n",
    "#pickle.dump(obj, file, protocol) \n",
    "#注释： \n",
    "#obj——序列化对象，将对象obj保存到文件file中去； \n",
    "#file——file表示保存到的类文件对象，file必须有write()接口，file可以是一个以’w’打开的文件或者是一个StringIO对象，也可以是任何可以实现write()接口的对象；\n",
    "#protocol——序列化模式，默认是 0（ASCII协议，表示以文本的形式进行序列化），protocol的值还可以是1和2（1和2表示以二进制的形式进行序列化。其中，1是老式的二进制协议；2是新二进制协议）。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c2d67",
   "metadata": {},
   "source": [
    "# 例二：回归计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82b74a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "DE_r = pd.read_excel('Training_Set.xlsx',sheet_name=1)\n",
    "array_r = DE_r.values\n",
    "X_r = array_r[:,2:138]\n",
    "Y_r = array_r[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "940bcf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test split\n",
    "X_train_r, X_test_r, Y_train_r, Y_test_r = train_test_split(X_r, Y_r, test_size=0.1, random_state=15, shuffle=True)\n",
    "#preprocessing\n",
    "scaler_r = preprocessing.StandardScaler().fit(X_train_r)\n",
    "X_train_r = scaler_r.transform(X_train_r)\n",
    "X_test_r = scaler_r.transform(X_test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8c9b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model construction\n",
    "regression = SVR(kernel='rbf',C=10, epsilon=0.1, gamma= 0.01).fit(X_train_r, Y_train_r)\n",
    "predicted_Y1_r = regression.predict(X_train_r)\n",
    "predicted_Y_r = regression.predict(X_test_r)\n",
    "#save model\n",
    "regression_model=pickle.dumps(regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0978cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Composition    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pd.read_excel('to_predict.xlsx')\n",
    "prediction.head()\n",
    "prediction.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e13c5",
   "metadata": {},
   "source": [
    "# 描述符的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffef2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorize_Formula:\n",
    "\tdef __init__(self):\n",
    "\t\telem_dict = pd.read_excel(r'elements.xlsx')\n",
    "\t\tself.element_df = pd.DataFrame(elem_dict) \n",
    "\t\tself.element_df.set_index('Symbol',inplace=True)\n",
    "\t\tself.column_names = []\n",
    "\t\tfor string in ['avg','diff','max','min']:\n",
    "\t\t\tfor column_name in list(self.element_df.columns.values):\n",
    "\t\t\t\tself.column_names.append(string+'_'+column_name)\n",
    "\n",
    "\tdef get_features(self, formula):\n",
    "\t\ttry:\n",
    "\t\t\tfractional_composition = mg.Composition(formula).fractional_composition.as_dict()\n",
    "#\t\t\telement_composition = mg.Composition(formula).element_composition.as_dict()\n",
    "\t\t\tavg_feature = np.zeros(len(self.element_df.iloc[0]))\n",
    "#\t\t\tsum_feature = np.zeros(len(self.element_df.iloc[0]))\n",
    "\t\t\tfor key in fractional_composition:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tavg_feature += self.element_df.loc[key].values * fractional_composition[key]                  \n",
    "\t\t\t\t\tdiff_feature = self.element_df.loc[list(fractional_composition.keys())].max()-self.element_df.loc[list(fractional_composition.keys())].min()\n",
    "\t\t\t\texcept Exception as e: \n",
    "\t\t\t\t\tprint('The element:', key, 'from formula', formula,'is not currently supported in our database')\n",
    "\t\t\t\t\treturn np.array([np.nan]*len(self.element_df.iloc[0])*4)\n",
    "\t\t\tmax_feature = self.element_df.loc[list(fractional_composition.keys())].max()\n",
    "\t\t\tmin_feature = self.element_df.loc[list(fractional_composition.keys())].min()\n",
    "\n",
    "\t\t\tfeatures = pd.DataFrame(np.concatenate([avg_feature, diff_feature, np.array(max_feature), np.array(min_feature)]))\n",
    "\t\t\tfeatures = np.concatenate([avg_feature, diff_feature, np.array(max_feature), np.array(min_feature)])\n",
    "\t\t\treturn features.transpose()\n",
    "\t\texcept:\n",
    "\t\t\tprint('There was an error with the Formula: '+ formula + ', this is a general exception with an unkown error')\n",
    "\t\t\treturn [np.nan]*len(self.element_df.iloc[0])*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b91c6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf=Vectorize_Formula()\n",
    "features=[]\n",
    "targets=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c867d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "for formula in prediction['Composition']:\n",
    "    features.append(gf.get_features(formula))\n",
    "X = pd.DataFrame(features, columns = gf.column_names)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "X_c = scaler_c.transform(X)\n",
    "pred_c = pickle.loads(classification_model)\n",
    "result_c = pred_c.predict(X_c)\n",
    "X_r = scaler_r.transform(X)\n",
    "pred_r = pickle.loads(regression_model)\n",
    "result_r = pred_r.predict(X_r)\n",
    "result=[]\n",
    "for i in range(len(result_c)):\n",
    "    if result_c[i] == 1:\n",
    "        result.append(result_r[i]);\n",
    "    else:\n",
    "        result.append(result_c[i])\n",
    "result = np.around(result,decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad31d579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0\n",
      "0  5.21\n",
      "1  3.57\n",
      "2  0.00\n",
      "3  0.00\n",
      "4  0.00\n",
      "5  5.73\n",
      "6  0.74\n",
      "7  1.48\n",
      "8  0.38\n",
      "A file named predicted.xlsx has been generated.\n",
      "Please check your folder.\n"
     ]
    }
   ],
   "source": [
    "composition=pd.read_excel('to_predict.xlsx',sheet_name='Sheet1', usecols=\"A\")\n",
    "#print(composition)\n",
    "composition=pd.DataFrame(composition)\n",
    "#print(composition)\n",
    "result=pd.DataFrame(result)\n",
    "print(result)\n",
    "predicted=np.column_stack((composition,result))\n",
    "predicted=pd.DataFrame(predicted)\n",
    "predicted.to_excel('predicted.xlsx', index=False, header=(\"Composition\",\"Predicted Eg\"))\n",
    "print(\"A file named predicted.xlsx has been generated.\\nPlease check your folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd180b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
